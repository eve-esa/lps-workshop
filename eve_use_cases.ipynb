{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use Case Demonstration of EVE\n",
    "In this notebook, we explore practical applications of EVE (Earth Virtual Explorer), a large language model specialized in Earth Observation (EO). EVE is designed to understand, analyze, and generate text related to EO data and topics, making it a valuable tool for researchers, analysts, and decision-makers in the field.\n",
    "\n",
    "## Overview\n",
    "We will demonstrate two key use cases of EVE:\n",
    "\n",
    "- **Summarization**: In this task, EVE is given a document related to Earth Observation and asked to generate a concise and informative summary. This is useful for quickly understanding lengthy reports, scientific papers, or satellite data documentation.\n",
    "\n",
    "- **Question Answering (Q&A)**: In this use case, we enhance EVE’s performance by integrating it with a retrieval system, using a technique known as Retrieval-Augmented Generation (RAG). Here, the model first retrieves relevant context from a knowledge base before answering questions, leading to more accurate and grounded responses.\n",
    "\n",
    "These examples highlight EVE’s potential to streamline information processing and support decision-making in Earth Observation workflows.\n",
    "\n",
    "Before diving into the use case, let’s briefly explore the core idea behind Large Language Models (LLM) and Natural Language Processing (NLP)."
   ],
   "metadata": {
    "id": "qh_1LMP-T5-E"
   },
   "id": "qh_1LMP-T5-E"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Natural Language Processing\n",
    "NLP is a field of linguistics and machine learning focused on understanding everything related to human language. The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context of those words. Some classic tasks in NLP are the following\n",
    "- **Classifying whole sentences**: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not\n",
    "- **Classifying each word in a sentence**: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization)\n",
    "- **Generating text content**: Completing a prompt with auto-generated text, filling in the blanks in a text with masked word\n",
    "- ..."
   ],
   "metadata": {
    "id": "6LAVsV0Vxtmy"
   },
   "id": "6LAVsV0Vxtmy"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Large Language Models (LLM)\n",
    "In recent years, the field of NLP has been revolutionized by Large Language Models (LLMs).\n",
    "LLM are a powerful subset of NLP models characterized by their massive size, extensive training data, and ability to perform a wide range of language tasks with minimal task-specific training.\n",
    "\n",
    "The objective of an LLM and more generally of a Language Model is to compute the probability of a series of words/tokens\n",
    "\n",
    "1. **Chain rule**: probability of a sequence is the product of conditional probabilities\n",
    "$$P\\left(w_1^m\\right)=\\prod_{n=1}^m P\\left(w_n \\mid w_1^{n-1}\\right)$$\n",
    "2. **Markov assumption**: N-gram model approximates it by conditioning only on the last N−1 words:\n",
    "\n",
    "$$P\\left(w_n \\mid w_1^{n-1}\\right) \\approx P\\left(w_n \\mid w_{n-N+1}^{n-1}\\right)$$\n",
    "\n",
    "So in general for **N-grams**\n",
    "$$P\\left(w_1^m\\right) \\approx P\\left(w_1^N\\right) \\cdot \\prod_{n=N+1}^M P\\left(w_n \\mid w_{n-N+1}^{n-1}\\right)$$\n",
    "\n",
    "**Example** for trigrams (N=3)\n",
    "$$P(\\text { country roads }) \\approx P(\\text { cou }) \\cdot P(n \\mid o u) \\cdot P(t \\mid u n) \\cdot P(r \\mid n t) \\cdot P(y \\mid t r) \\cdot P(\\mid r y) \\cdot \\text { etc. }$$\n",
    "\n",
    "Now let's load a LLM from HuggingFace"
   ],
   "metadata": {
    "id": "gAPWwur7yAZs"
   },
   "id": "gAPWwur7yAZs"
  },
  {
   "cell_type": "code",
   "source": [
    "# Install the required libraries (already installed)\n",
    "#!pip3 install -q datasets\n",
    "#!pip3 install -q langchain_community\n",
    "#!pip3 install -q pypdf\n",
    "#!pip3 install -q sentence-transformers\n",
    "#!pip3 install -q faiss-cpu\n",
    "#!pip3 install -q langchain_huggingface\n",
    "#!pip3 install -q langchain_runpod\n",
    "#!pip3 install -q qdrant_client\n",
    "#!pip3 install -q langchain_aws\n",
    "#!pip3 install evaluate\n",
    "#!pip3 install rouge_score\n",
    "#!pip3 install bert_score\n",
    "#!pip3 install numpy==1.26.4"
   ],
   "metadata": {
    "id": "pVibg-0CjKph"
   },
   "id": "pVibg-0CjKph",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup env\n",
    "from dotenv import dotenv_values\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "config = dotenv_values('/home/sagemaker-user/env.sh')"
   ],
   "metadata": {
    "id": "ANlj1YkSqG5T"
   },
   "id": "ANlj1YkSqG5T",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')"
   ],
   "metadata": {
    "id": "sW_vjEG63aAF"
   },
   "id": "sW_vjEG63aAF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before feeding our LLM with our question we need to tokenize the question. The tokenization is the process of breaking into pieces a text, this pieces are called token. Subword Tokenization is one of the most popular and efficent tokenization methods,  a word is split into subwords and these subwords are known as tokens, For example the word “football” might be split into “foot”, and “ball”.\n",
    "\n",
    "Let's load a tokenizer and see how it works.\n"
   ],
   "metadata": {
    "id": "6ILU6EGo5-jS"
   },
   "id": "6ILU6EGo5-jS"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')\n",
    "message = 'The European Space Agency (ESA) is a 23-member international organization devoted to space exploration'\n",
    "\n",
    "# Tokenize with options to return offsets and word_ids\n",
    "encoded = tokenizer(\n",
    "    message,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"])\n",
    "# The _ denotes a space before the word\n",
    "print(tokens)"
   ],
   "metadata": {
    "id": "QCTkVNCQ7Jyy"
   },
   "id": "QCTkVNCQ7Jyy",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompting\n",
    "Prompting is the process of providing input to a language model in the form of a carefully designed instruction or question to guide its output.\n",
    "\n",
    "First, we will define the prompt to be used using three different parts:\n",
    "- **System message**: this section provides guidelines for the model on how to behave and interpret the conversation.\n",
    "- **Human message**: this section contains the input or message from the user.\n",
    "- **AI message**: this section represents a response generated by the model.\n",
    "\n",
    "From the code below we can see the structure of the prompt and the templates used to create it. Specifically,. Specifically we could see some **special tokens** used in the prompt:\n",
    "- **<|system|> | <|user|> | <|assistant|>**: are special tokens that helps the model to understand to who belongs that specific message.\n",
    "- **<|end|>**: is a special token that indicates the end of a message.\n",
    "- **{message}**: is a placeholder that will be replaced with the actual message, context and question."
   ],
   "metadata": {
    "id": "kbxBwRpTGpxU"
   },
   "id": "kbxBwRpTGpxU"
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"<|system|>\n",
    "You are a helpful chatbot, be kind and answer to the user questions.<|end|>\n",
    "<|user|>\n",
    "{message}\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "c2dDXdqXH9g2"
   },
   "id": "c2dDXdqXH9g2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "message = 'What is ESA?'\n",
    "prompt = prompt.format(message=message)"
   ],
   "metadata": {
    "id": "cLDuAq0oIlR7"
   },
   "id": "cLDuAq0oIlR7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "# Let's assemble our pipeline\n",
    "# Build the text generation pipeline\n",
    "llama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"  # uses GPU if available\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfoEVD0bDDX3",
    "outputId": "a4a81c7a-6c2f-4793-f297-57af83acc6cd"
   },
   "id": "CfoEVD0bDDX3",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "output = llama_pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7)"
   ],
   "metadata": {
    "id": "MMbhcJURDyrc"
   },
   "id": "MMbhcJURDyrc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Here we can see the whole conversation with the generated answer by the model\n",
    "print(output[0]['generated_text'])"
   ],
   "metadata": {
    "id": "GNFuyMRvD4fg"
   },
   "id": "GNFuyMRvD4fg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarization\n",
    "\n",
    "In this section, we will ask our model to perform text summarization giving as a document a paper from our dataset."
   ],
   "metadata": {
    "id": "udcAFT-wWzlc"
   },
   "id": "udcAFT-wWzlc"
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import  load_dataset\n",
    "import random\n",
    "# Load documents\n",
    "docs = load_dataset('eve-esa/summarization_ds_10k_sample_split')['dev']\n",
    "\n",
    "# Select a random doc from the dataset\n",
    "doc = docs.select(random.sample(range(len(docs)), 1))[0]"
   ],
   "metadata": {
    "id": "ZkmZqeqPWuEF"
   },
   "id": "ZkmZqeqPWuEF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Passage that we are going to summarize\n",
    "\n",
    "passage = \"\"\"### Minimum Volume Ellipsoid and SDP Formulation for Approximating \\\\(W^{-1}\\\\)\n",
    "\n",
    "As explained in the previous section, we need to find a matrix \\\\(Q\\\\) such that \\\\(\\\\kappa(QW)\\\\) is close to one. Let \\\\(A=Q^{T}Q\\\\succ 0\\\\). We have \\\\(W^{T}AW=(QW)^{T}(QW)\\\\) so that \\\\(\\\\sigma_{i}(W^{T}AW)=\\\\sigma_{i}^{2}(QW)\\\\) for all \\\\(i\\\\), hence it is equivalent to find a matrix \\\\(A\\\\) such that \\\\(\\\\kappa(W^{T}AW)\\\\) is close to one since it will imply that \\\\(\\\\kappa(QW)\\\\) is close to one, while we can compute a factorization of \\\\(A=Q^{T}Q\\\\) (e.g., a Cholesky decomposition). Ideally, we would like that \\\\(A=(WW^{T})^{-1}=W^{-T}W^{-1}\\\\), that is, \\\\(Q=RW^{-1}\\\\) for some orthonormal transformation \\\\(R\\\\).\n",
    "\n",
    "The central step of our algorithm is to compute the minimum volume ellipsoid centered at the origin containing all columns of \\\\(\\\\tilde{M}\\\\). An ellipsoid \\\\(\\\\mathcal{E}\\\\) centered at the origin in \\\\(\\\\mathbb{R}^{r}\\\\) is described via a positive definite matrix \\\\(A\\\\in\\\\mathbb{S}_{++}^{r}\\\\) :\n",
    "\n",
    "\\\\[\\\\mathcal{E}=\\\\{\\\\ x\\\\in\\\\mathbb{R}^{r}\\\\ |\\\\ x^{T}Ax\\\\leq 1\\\\ \\\\}.\\\\]\n",
    "\n",
    "The axes of the ellipsoid are given by the eigenvectors of matrix \\\\(A\\\\), while their length is equal to the inverse of the square root of the corresponding eigenvalue. The volume of \\\\(\\\\mathcal{E}\\\\) is equal to \\\\(\\\\det(A)^{-1/2}\\\\) times the volume of the unit ball in dimension \\\\(r\\\\). Therefore, given a matrix \\\\(\\\\tilde{M}\\\\in\\\\mathbb{R}^{r\\\\times n}\\\\) of rank \\\\(r\\\\), we can formulate the minimum volume ellipsoid centered at the origin and containing the columns \\\\(\\\\tilde{m}_{i}\\\\)\\\\(1\\\\leq i\\\\leq n\\\\) of matrix \\\\(\\\\tilde{M}\\\\) as follows\n",
    "\n",
    "\\\\[\\\\min_{A\\\\in\\\\mathbb{S}_{++}^{r}}\\\\ \\\\log\\\\det(A)^{-1}\\\\quad\\\\text{such that}\\\\quad\\\\ \\\\tilde{m_{i}}^{T}A\\\\tilde{m}_{i}\\\\leq 1\\\\quad\\\\text{ for }i=1,2,\\\\ldots,n. \\\\tag{1}\\\\]\n",
    "\n",
    "This problem is SDP representable [9, p.222] (see also Remark 10). Note that if \\\\(\\\\tilde{M}\\\\) is not full rank, that is, the convex hull of the columns of \\\\(\\\\tilde{M}\\\\) and the origin is not full dimensional, then the objective function value is unbounded below. Otherwise, the optimal solution of the problem exists and is unique [19].\"\"\""
   ],
   "metadata": {
    "id": "m5RmL9rR_0f3"
   },
   "id": "m5RmL9rR_0f3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "# Load the model using APIs\n",
    "llm = BedrockLLM(model_id='arn:aws:bedrock:us-west-2:637423382292:imported-model/81wcbcqx5vro', region_name='us-west-2', provider='meta')"
   ],
   "metadata": {
    "id": "Nu1eHm5tXoq_"
   },
   "id": "Nu1eHm5tXoq_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt message\n",
    "\n",
    "message = f\"\"\"\n",
    "<|system|>\n",
    "You are an helpful assistant expert in Earth Observation, help the user with his tasks.\n",
    "<|end|>\n",
    "<|user|>\n",
    "Summarize the following document focusing on the main concepts and ideas.\n",
    "\n",
    "The document starts here:\n",
    "{passage}\n",
    "\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "uSYFscIPfgSM"
   },
   "id": "uSYFscIPfgSM",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run this cell to show the final prompt given to the model\n",
    "display(Latex(message))"
   ],
   "metadata": {
    "id": "wI7y0uBl0nDP"
   },
   "id": "wI7y0uBl0nDP",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output = llm.invoke(message)"
   ],
   "metadata": {
    "id": "tKyDu4JKedrX"
   },
   "id": "tKyDu4JKedrX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "display(Latex(message))",
   "metadata": {
    "id": "6B6G3af6ciOM"
   },
   "id": "6B6G3af6ciOM",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "6c07c15e489c973a"
   },
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Generation (RAG) with Langchain\n",
    "\n",
    "In this section, we implement a complete RAG pipeline for answering questions based on a given context. Using the LangChain library, we'll walk through the entire process—from retrieving relevant context to generating accurate answers.\n"
   ],
   "id": "6c07c15e489c973a"
  },
  {
   "metadata": {
    "id": "b793ab84cc858c7f"
   },
   "cell_type": "markdown",
   "source": [
    "**Roadmap**\n",
    "\n",
    "1. **Indexing**: Organize the raw documents into a structured format suitable for processing, such as splitting them into chunks or passages for more efficient retrieval.\n",
    "\n",
    "2. **Embedding**: Convert each text chunk into a dense vector representation using a pre-trained embedding model. These embeddings capture the semantic meaning of the content.\n",
    "\n",
    "3. **Vector Store**: Store the embeddings in a vector database (Qdrant in our case), allowing fast and scalable similarity search across the document collection.\n",
    "\n",
    "4. **Retrieval and Generation**: Given a user query, retrieve the most relevant document chunks from the vector store and feed them into a language model (EVE) to generate a context-aware, accurate response."
   ],
   "id": "b793ab84cc858c7f"
  },
  {
   "metadata": {
    "id": "6f19179e8af8ce6a"
   },
   "cell_type": "markdown",
   "source": [
    "## Load dataset of Q&A\n",
    "Let's load our dataset of Q&A about EO, each sample is composed of a question and an answer"
   ],
   "id": "6f19179e8af8ce6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T10:21:01.031700Z",
     "start_time": "2025-03-24T10:20:56.045973Z"
    },
    "id": "b733b528b619214"
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "qa = load_dataset('eve-esa/eve-is-open-ended')['train']\n",
    "\n",
    "# Take a random idx\n",
    "idx = 120\n",
    "\n",
    "question = qa[idx]['question']\n",
    "answer = qa[idx]['answer']\n",
    "\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Answer: ', answer)"
   ],
   "id": "b733b528b619214",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1fe9e4d1eed5cb3"
   },
   "cell_type": "markdown",
   "source": [
    "## Indexing\n",
    "\n",
    "The first part of a RAG pipeline is called **indexing**. This is the process of ingesting data from a source and indexing it. The indexing process is composed of three steps:\n",
    "- **Load**: process and load data in text format.\n",
    "- **Split**: this is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "- **Store**: we need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models/) model.\n",
    "Once the Indexing step is done we will have our knowledge base made of scientific papers indexed and ready to be used in the generation steps as context.\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\" width=\"800\"/>\n",
    "<figcaption><a href='https://python.langchain.com/docs/tutorials/rag/'>Source</a></figcaption>\n",
    "</figure>\n"
   ],
   "id": "1fe9e4d1eed5cb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chunking is a fundamental step in working with large language models, especially in retrieval-augmented generation (RAG) pipelines. It refers to the process of splitting long documents or texts into smaller, manageable pieces—called chunks—that can be efficiently stored, indexed, and retrieved when needed. Well-formed chunks are crucial because they ensure that each piece of text contains enough context to be meaningful on its own, which in turn improves the quality of retrieval and the relevance of the information provided to the language model. Poorly chunked text can lead to incomplete or confusing results, so careful design of the chunking strategy is essential.\n"
   ],
   "metadata": {
    "id": "WA8RIU9ZLK2f"
   },
   "id": "WA8RIU9ZLK2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we are working with structured documents, we want to avoid blindly splitting the text based solely on character or token length. Instead, we can take advantage of the document’s markdown hierarchy to guide our chunking strategy. By doing so, each chunk will correspond to a logical section of the document—such as a heading and its associated content—making the chunks more coherent and meaningful. This structure-aware approach improves the quality of retrieval and the relevance of the context passed to the language model. We will use LangChain to implement this custom splitting logic efficiently."
   ],
   "metadata": {
    "id": "NiD0_W7tM1V6"
   },
   "id": "NiD0_W7tM1V6"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\")\n",
    "]\n",
    "\n",
    "markdown_document = \"\"\"###### Abstract\n",
    "\n",
    "Total nitrogen concentration (C\\({}_{\\text{TN}}\\)) enrichment is the primary cause of natural water eutrophication. Accurately estimating C\\({}_{\\text{TN}}\\) and its spatiotemporal dynamics is crucial for formulating monitoring and control measures to alleviate lake eutrophication. A hybrid model was proposed for estimating C\\({}_{\\text{TN}}\\) in optically complex inland waters by incorporating the relationship between C\\({}_{\\text{TN}}\\) and water optical active components for Zhuhai-1 Orbita hyperspectral (OHS) imagery. Compared with other semi-analytical algorithms, the re-adjusted reference wavelength QMA's shows the best performance in a(a) retrieval. The hybrid model for C\\({}_{\\text{TN}}\\) estimation achieves an root mean square deviation (RMSD) of 0.20 mg/L, a mean absolute percentage deviation (MAPD) of 6.96 %, and a unbiased mean absolute percentage deviation (UMAPD) of 6.96 %, with a\\({}_{\\text{DM}}\\)(569) accuracy exerting the greatest influence. Ground-satellite synchronous validation demonstrates robust performance, with an RMSD of 0.28 mg/L, a MAPD of 14.49 %, and a UMAPD of 14.92 %. The hybrid model was applied to OHS observations of Lake Dinch from April 2019 to September 2021. The analysis revealed a generally decreasing trend in C\\({}_{\\text{TN}}\\) during this timeframe. The above results demonstrate that the robustness and applicability of the proposed C\\({}_{\\text{TN}}\\) hybrid model for inland waters with complex optical properties. Furthermore, satellite-based data products provide valuable information for formulating lake management strategies.\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Freshwater lakes serve as crucial ecosystems that significantly influence global climate regulation, maintain biodiversity, and provide essential resources for humans (Ho et al., 2019; Li et al., 2024). Human activities, including agricultural runoff and industrial waste discharge, coupled with the effects of global climate change, are causing widespread eutrophication and shrinkage of lakes worldwide (Dai et al., 2023; Grant et al., 2021). Excessive nitrogen elements in aquatic ecosystems have severe adverse effects on aquatic organisms and humans, such as eutrophication, water acidification, and toxicity (Li et al., 2023; Sheikholeslami and Hall, 2023). Therefore, obtaining total nitrogen concentration (C\\({}_{\\text{TN}}\\)) and its spatial distribution is critical for furthering our understanding of the eutrophication process and biogeochemicalprocess of aquatic ecosystems.\n",
    "\n",
    "Lake Bianchi is the the largest freshwater lake in Yunnan Province of China. It plays a crucial role in the region's ecological balance, water supply, and economic development (Li et al., 2024; Zheng et al., 2023). However, like other reservoirs and lakes around the world, the lake has suffered from severe environmental degradation in recent decades (Feng et al., 2021; Ho et al., 2019). Since the 1970 s, Lake Bianchi has experienced prolonged algal blooms due to increased nitrogen and phosphorus loads, severely affecting the lake's aquatic ecology and the likelihoods of surrounding residents (Cheng et al., 2023; Mu et al., 2021). Therefore, monitoring nutrient concentrations, especially total nitrogen, is crucial for the management of lake eutrophication.\n",
    "\n",
    "Traditionally, \\(\\mathrm{C_{IN}}\\) monitoring and assessment have relied on field measurements and laboratory analysis (Wang et al., 2022). While these methods provide valuable data points, they are inherently limited in their ability to capture large-scale spatial and temporal variations in \\(\\mathrm{C_{IN}}\\). Additionally, they are both labor-intensive and time-consuming to implement (Cai et al., 2023; Li et al., 2024). Earth observation techniques, with their wide spatial coverage and capability for long-term, continuous monitoring, offer significant potential for uninterrupted water quality monitoring (Liu et al., 2020; Zheng et al., 2023). However, accurately estimating \\(\\mathrm{C_{IN}}\\) remains challenging. Unlike optically active substances (OACs), \\(\\mathrm{C_{IN}}\\) lacks a distinct spectral signature within the detectable wavelength range, and TN is therefore referred to as a non-optically active substance (Li et al., 2023; Li et al., 2017). Therefore, estimating \\(\\mathrm{C_{IN}}\\) in inland entropic waters using the spectral features of remote sensing is difficult.\n",
    "\n",
    "In the last two decades, researchers have achieved substantial advancements in \\(\\mathrm{C_{IN}}\\) estimation using remote sensing techniques, primarily through direct and indirect estimation models (Chen and Quan, 2012; Dong et al., 2020; Guo et al., 2021; Guo et al., 2022; Li et al., 2023; Li et al., 2017; Qun'ou et al., 2021; Torbick et al., 2013; Wang et al., 2020; Zhu et al., 2023). Direct estimation models establish statistical relationships between remote sensing reflectance (\\(\\mathrm{R_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)) and \\(\\mathrm{C_{IN}}\\). Multiple linear regression models have been employed for estimating \\(\\mathrm{C_{IN}}\\) in various water bodies, such as Lake Taihu, the Pearl River Estuary, and the Lower Peninsula of Michigan (Chen and Quan, 2012; Guo et al., 2022; Torbick et al., 2013). Beyond traditional methods, researchers have increasingly focused on leveraging machine learning algorithms for \\(\\mathrm{C_{IN}}\\) estimation. Examples include artificial neural networks, multi-spectral scale morphological combined features, support vector machines, AdaBoost Regression, and Gradient Boosting Regression, which have been trained and successfully applied to \\(\\mathrm{C_{IN}}\\) estimation in inland rivers and lakes (Guo et al., 2021; Li et al., 2023; Zhu et al., 2023). While direct retrieval models offer advantages in terms of simplicity and ease of use, their effectiveness and applicability across diverse water bodies are significantly constrained by limitations in data availability and the complexity of the relationship between \\(\\mathrm{C_{IN}}\\) and the selected features (Cai et al., 2023). Additionally, the direct use of \\(\\mathrm{R_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\)) to estimate \\(\\mathrm{C_{IN}}\\) is controversial, as TN is a non-optically active substance, and changes in \\(\\mathrm{C_{IN}}\\) are difficult to reflect in \\(\\mathrm{R_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(Li et al., 2023). The second type involves indirect estimation models that leverage the relationships between \\(\\mathrm{C_{IN}}\\) and OACs. Chen and Quan (2012) constructed a multiple linear regression model using OACs as predictor variables and \\(\\mathrm{C_{IN}}\\) as the response variable. This model is applicable to \\(\\mathrm{C_{IN}}\\) estimation in Lake Taihu. Previous empirical models only used band ratios or band combination models to estimate \\(\\mathrm{C_{IN}}\\), and such models are difficult to directly apply to different lakes or different periods within the same lake (Oyama et al., 2009). Compared to direct estimation models, indirect models offer the advantage of incorporating bio-optical relationships between \\(\\mathrm{C_{IN}}\\) and OACs. Therefore, we attempted to develop a novel \\(\\mathrm{C_{IN}}\\) estimation model that considers the relationship between \\(\\mathrm{C_{IN}}\\) and the inherent optical properties of water bodies.\n",
    "\n",
    "In optically complex waters, various constituents like phytoplankton, suspended solids, and colored dissolved organic matter (CDOM) collectively modulate the water's spectral signature, making it challenging to isolate the signal specific to \\(\\mathrm{C_{IN}}\\)(Xue et al., 2019). Additionally, the absorption coefficients of phytoplankton (\\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\))), non-algal particles (\\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\))), and colored dissolved organic matter (\\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\))) quantify how these optically active constituents absorb light particles, thereby reducing the light energy that penetrates the water column. These coefficients serve as the connection between \\(\\mathrm{R_{\\mathrm{\\SIUnitSymbolMicro m}}}\\) and various OACs. Consequently, they are frequently incorporated as intermediary variables within models for estimating various water quality parameters (Liu et al., 2020; Zheng et al., 2023). Researchers have proposed various estimation models to estimate these absorption coefficients. These models encompass a range of approaches, including empirical approaches and semi-analytical approaches (Huang et al., 2014; Lee et al., 2014; Liu et al., 2020; Xue et al., 2019; Zheng et al., 2023). Building upon existing algorithms like QAA.V5, researchers have developed improved versions specifically designed for different water types. Lee et al. (2014) introduced QAA.V6 for clear ocean applications, utilizing a 670 nm reference wavelength and recalibrated coefficients. This model has proven effective in retrieving inherent optical properties (IOPs) of clear oceans (Jiang et al., 2019; Jorge et al., 2021). For inland turbid waters, modifications like Huang et al. (2014) obtain (using a 710 nm reference wavelength) and Xue et al. (2019) obtain \\(\\mathrm{QAA_{750}}\\) algorithm (using a 750 nm reference wavelength) were developed to address retrieval challenges in these environments. Zheng et al. (2023) proposed a QAA algorithm (\\(\\mathrm{QAA_{710}}\\)) with a reference wavelength of 716 nm and successfully estimated the total absorption coefficient (\\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)) and \\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\)) of Lake Tianchi. These advancements not only enabled IOP retrieval but also facilitated the successful estimation of chlorophyll-a (Chla) and total suspended matter (TSM), contributing to a more comprehensive assessment of inland water quality. Previous studies have demonstrated that using absorption coefficients, rather than OACs, is a more effective and feasible approach for estimating \\(\\mathrm{C_{IN}}\\) in optically complex inland waters (Shi et al., 2019; Zhang et al., 2018). Therefore, we attempted to develop a model based on \\(\\mathrm{C_{IN}}\\) and absorption coefficients.\n",
    "\n",
    "This study focused on developing a hybrid remote sensing algorithm specifically designed to estimate \\(\\mathrm{C_{IN}}\\) in inland waters with complex optical properties. The proposed hybrid algorithm integrates the improved QAA algorithm and semi-empirical algorithms to estimate absorption coefficients and their relationship with \\(\\mathrm{C_{IN}}\\). The Zhuhai-1 Orbita hyperspectral (OHS) satellite has emerged as a valuable asset for monitoring \\(\\mathrm{C_{IN}}\\) in inland waters. This next-generation satellite boasts exceptional spatial (10 m) and temporal resolution (2.5 days) alongside a high spectral resolution (32 spectral bands). This unique combination allows for the capture of fine-scale spatial and temporal variations in \\(\\mathrm{C_{IN}}\\) across inland aquatic ecosystems. Specifically, we took the following steps: (1) proposed a hybrid model for estimating \\(\\mathrm{C_{IN}}\\) based on absorption coefficients in inland waters; (2) employed the QAA\\({}_{716}\\) model to derive target absorption coefficients; (3) obtained the spatiotemporal variation of \\(\\mathrm{C_{IN}}\\) in Lake Tianchi, a typical eutrophic lake; (4) evaluated the feasibility of using OHS data to estimate \\(\\mathrm{C_{IN}}\\) and conduct a radiation performance assessment.\n",
    "\n",
    "## 2 Study area and data\n",
    "\n",
    "### Study region\n",
    "\n",
    "Lake Bianchi, the largest freshwater lake on the Yunnan-Guizhou Plateau, plays a vital role in the regional ecosystem and economy (Fig. 1). It covers an area of approximately 330 km\\({}^{2}\\) with a mean depth of 4.4 m (Huang et al., 2014). Lake Tianchi is a typical tectonic lake with numerous inflowing tributaries and only one outflowing river system, resulting in distinct closed-semi-closed characteristics. Over the past few decades, rapid urbanization and industrial development have led to a sharp deterioration in water quality, with large amounts of nitrogen and phosphorus nutrients being discharged into the lake (Li et al., 2023). This has resulted in increasing eutrophication of the lake,leading to large-scale algal blooms under suitable conditions, posing a serious threat to Kunming's tourism industry (Li et al., 2024).\n",
    "\n",
    "### Field data\n",
    "\n",
    "To capture seasonal variations in water quality and optical properties, field campaigns were conducted in Lake Dianchi during April and November 2017 (Fig. 1B & Table 1).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n"
   ],
   "metadata": {
    "id": "GfH_zVHPLJxS"
   },
   "id": "GfH_zVHPLJxS",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for header_split in md_header_splits:\n",
    "  headers = \"\\n\".join([value for value in header_split.metadata.values()])\n",
    "  print(f'Section name: ', headers)\n",
    "  print('Chunk content:\\n', header_split.page_content)\n",
    "  print()"
   ],
   "metadata": {
    "id": "B7W0CyvUOC2V"
   },
   "id": "B7W0CyvUOC2V",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "1e6fe2ba335ad8fe"
   },
   "cell_type": "markdown",
   "source": [
    "### Embeddings\n",
    "\n",
    "An embeddings model in Retrieval-Augmented Generation (RAG) is a neural network that converts text into dense vector representations (embeddings) in a **high-dimensional space**. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\n",
    "\n",
    "Embeddings models are trained on large text corpora using unsupervised learning techniques. They learn to encode the semantic meaning of words, sentences, and documents in a way that captures relationships between them. For example, embeddings models can learn that \"cat\" and \"dog\" are similar because they are both animals, or that \"apple\" and \"orange\" are similar because they are both fruits.\n",
    "\n",
    "There are many pre-trained embedding models available, each suited to different types of data and use cases. For our application, we use [Indus](https://huggingface.co/papers/2405.10725), a fine-tuned encoder-only transformer model trained specifically on scientific journals and articles related to NASA’s Science Mission Directorate (SMD).\n",
    "\n",
    "Choosing the right embedding model is a critical step in building an effective retrieval system. Ideally, the embedding model should be trained—or at least fine-tuned—on data similar to the target documents. Since our corpus consists of scientific texts focused on Earth Observation, Indus is a better fit than a general-purpose model, as it captures domain-specific terminology and semantics more accurately.\n",
    "<figure>\n",
    "\n",
    "<img src=\"https://weaviate.io/assets/images/embedding-models-0c04d93c0be28dd63a0e8781c4e8685d.jpg\" width='800px'>\n",
    "<figcaption><a href='https://weaviate.io/blog/how-to-choose-an-embedding-model'>Source</a></figcaption>\n",
    "<figure>\n",
    "\n"
   ],
   "id": "1e6fe2ba335ad8fe"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the embeddings model\n",
    "model_name = \"nasa-impact/nasa-smd-ibm-st-v2\"\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "indus_embd = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,  encode_kwargs=encode_kwargs\n",
    ")"
   ],
   "metadata": {
    "id": "XYz4pvs3jD7S"
   },
   "id": "XYz4pvs3jD7S",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "8746b6b79e3c4d83"
   },
   "cell_type": "markdown",
   "source": [
    "### Vector Store\n",
    "\n",
    "Vector stores are specialized databases designed to efficiently index and retrieve information using vector representations of data. Vector stores leverages the dense representation by reducing the task of finding similar documents to a search in a high-dimensional space. This search is made by comparing the vector representation of the **query** with the vector representation of the **documents** in the database. The documents that are closer to the query vector are considered more similar to the query.\n",
    "\n",
    "Wrapping up the retrieval process is composed of:\n",
    "- **Documents embedding**\n",
    "- **Store the embeddings in a VectorStore**\n",
    "- **Query embedding**\n",
    "- **Retrieve** the most similar documents to the query\n",
    "\n",
    "\n",
    "The most popular and simple setup is using the **cosine similarity** to compare the vectors and retrieve the **top k** most similar ones\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png\" width=\"800\"/>\n",
    "<figcaption><a href='https://python.langchain.com/docs/concepts/vectorstores/'>Source</a></figcaption>\n",
    "</figure>\n"
   ],
   "id": "8746b6b79e3c4d83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connect to QDrant\n",
    "\n",
    "To save time, the embedding and indexing of documents have already been completed prior to this notebook. These steps can be computationally intensive, so we’ve pre-processed the data to streamline the workflow.\n",
    "\n",
    "We are using Qdrant as our vector store, which has been preloaded with all the relevant documents needed for retrieval. In this section, we will connect to the Qdrant instance and select the specific collection that contains our indexed data. This will enable us to perform efficient semantic searches and support the Retrieval-Augmented Generation (RAG) process used in our Q&A tasks."
   ],
   "metadata": {
    "id": "HWgZw1keeYtq"
   },
   "id": "HWgZw1keeYtq"
  },
  {
   "cell_type": "code",
   "source": [
    "# Examples of retrieval pipeline using the embedding function and the API from QDrant\n",
    "from qdrant_client import QdrantClient\n",
    "import os\n",
    "\n",
    "qdrant_url = os.getenv('QDRANT_URL', config['QDRANT_URL'])\n",
    "api_key = os.getenv('QDRANT_API_KEY', config['QDRANT_API_KEY'])\n",
    "\n",
    "# Enstablish a connection wit the vector store\n",
    "client = QdrantClient(\n",
    "    url=qdrant_url,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Embedd the query\n",
    "query_emb = indus_embd.embed_query(question)\n",
    "\n",
    "# Perform similarity search using the computed embeddings\n",
    "search_result = client.search(\n",
    "    collection_name=\"esa-nasa-workshop\",\n",
    "    query_vector=query_emb,\n",
    "    limit=1,\n",
    ")\n",
    "\n",
    "data = search_result[0].payload\n",
    "# Payload containing metadata and text\n",
    "for key, value in data.items():\n",
    "  print(f'{key}: {value}')\n",
    "\n",
    "print('Retrieved chunk:\\n', data['text'])"
   ],
   "metadata": {
    "id": "OBzZdrljQXHv"
   },
   "id": "OBzZdrljQXHv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "from pydantic import PrivateAttr, Field\n",
    "from typing import List, Optional, Dict\n",
    "from qdrant_client.models import Filter, PointStruct\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Let's define our retriever class to have a nice interface\n",
    "class QdrantRetriever():\n",
    "    def __init__(self, embedding, api_key, qdrant_url, collection_name='esa-nasa-workshop', k: int = 3):\n",
    "        self._client = QdrantClient(url=qdrant_url, api_key=api_key)\n",
    "        self.embedding = embedding\n",
    "        self.collection_name = collection_name\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        query_emb = self.embedding.embed_query(query)\n",
    "\n",
    "        search_result = self._client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_emb,\n",
    "            limit=self.k,\n",
    "        )\n",
    "\n",
    "        docs = []\n",
    "        for hit in search_result:\n",
    "            # Adjust based on your actual data structure\n",
    "            data = hit.payload\n",
    "            content = data.get(\"text\", \"\")\n",
    "            metadata = {}\n",
    "            for key, value in data.items():\n",
    "                if key != \"text\":\n",
    "                    metadata[key] = value\n",
    "            docs.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "        return docs\n"
   ],
   "metadata": {
    "id": "3ZqrzBMFbcw9"
   },
   "id": "3ZqrzBMFbcw9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Let's define our retriever\n",
    "retriever = QdrantRetriever(indus_embd, api_key, qdrant_url, k=3)\n",
    "\n",
    "# Format retrieved documents:\n",
    "def format_docs(docs):\n",
    "  doc_str = ''\n",
    "  for i, doc in enumerate(docs):\n",
    "    doc_str += f'Document n. {i+1}\\n'\n",
    "    doc_str += f'TITLE: {doc.metadata.get(\"title\", \"No title\")}\\n' # Add title's of the paper\n",
    "    if type(doc.metadata.get(\"headers\", {})) is str:\n",
    "      # Parse JSON string\n",
    "      doc.metadata['headers'] = json.loads(doc.metadata.get(\"headers\", {}))\n",
    "    for key, value in doc.metadata.get(\"headers\", {}).items():\n",
    "      doc_str += f'{value}\\n'\n",
    "    doc_str += f'URL: {doc.metadata.get(\"url\", \"No url\")}\\n\\n' # Add URL of the paper\n",
    "    doc_str += f'{doc.page_content}\\n\\n'\n",
    "  return doc_str\n",
    "\n",
    "\n",
    "\n",
    "print('Question: ')\n",
    "print(question)\n",
    "print()\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "print(format_docs(docs))"
   ],
   "metadata": {
    "id": "ApDagHKExlZa",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:49:48.506800Z",
     "start_time": "2025-03-24T10:49:46.230385Z"
    }
   },
   "id": "ApDagHKExlZa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b019ad14ade15fcf"
   },
   "cell_type": "markdown",
   "source": [
    "## Retrieval and generation\n",
    "\n",
    "The pipeline consists of the following key components:\n",
    "\n",
    "- Retriever: This component queries the vector store (Qdrant) to fetch the most relevant document chunks based on the user’s question. It performs a semantic search using the pre-computed embeddings to find contextually similar content.\n",
    "\n",
    "- LLM (Large Language Model): Once the relevant context is retrieved, it is passed to EVE, our Earth Observation-specialized language model. EVE then generates a coherent and informed response based on both the query and the retrieved context.\n",
    "\n",
    "This approach ensures that the generated answers are grounded in the source documents, improving accuracy and reducing hallucination."
   ],
   "id": "b019ad14ade15fcf"
  },
  {
   "metadata": {
    "id": "a2e01ecdefdfa5a6"
   },
   "cell_type": "markdown",
   "source": [
    "### Prompt\n",
    "\n",
    "First, we will define the prompt to be used  using three different templates:\n",
    "- **SystemMessagePromptTemplate**: the system message represents guidelines for the model on how to interact with the user and interpret the conversation.\n",
    "- **AIMessagePromptTemplate**: the AI message represents a message generate by the model.\n",
    "- **HumanMessagePromptTemplate**: the human message represents the message sent by the user.\n"
   ],
   "id": "a2e01ecdefdfa5a6"
  },
  {
   "metadata": {
    "id": "18c906d0107239a5",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:50:44.566443Z",
     "start_time": "2025-03-24T10:50:44.481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os  # Customize SystemPromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template= '''<|system|>\n",
    "{message}\n",
    "<|end|>\n",
    "'''\n",
    "\n",
    "# A human message will contain the question and the context. The context will be automatically added by the retriever.\n",
    "human_template = '''<|user|>\n",
    "Context: {context}\n",
    "\n",
    "Question is below:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "\n",
    "assistant_template = '''<|assistant|>\n",
    "{message}\n",
    "<|end|>\n",
    "'''\n",
    "\n",
    "# Define the templates\n",
    "SystemMessageTemplate = SystemMessagePromptTemplate.from_template(template)\n",
    "HumanMessageTemplate = HumanMessagePromptTemplate.from_template(human_template)\n",
    "AIMessageTemplate = AIMessagePromptTemplate.from_template(assistant_template)\n"
   ],
   "id": "18c906d0107239a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "aa7dc3d79129b2d7",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:50:51.504971Z",
     "start_time": "2025-03-24T10:50:51.460937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the system message\n",
    "\n",
    "system_message = '''You are an expert assistant that answers questions about different topics.\n",
    "\n",
    "You are given some extracted parts from science papers along with a question.\n",
    "\n",
    "If you don't know the answer, just say \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "Do not use any prior knowledge.'''\n",
    "\n",
    "\n",
    "system_msg = SystemMessageTemplate.format(message=system_message)\n",
    "\n",
    "system_msg"
   ],
   "id": "aa7dc3d79129b2d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f2a969efb8649e95"
   },
   "cell_type": "markdown",
   "source": [
    "Now that we have the definition of different templates we can define the chat prompt. Langchain requireres a specific structure for the chat prompt that is composed of a list of messages. In the code below we can see that our chat template will be composed of two messages, the **system message** and the **human message** that contains the input from the user."
   ],
   "id": "f2a969efb8649e95"
  },
  {
   "metadata": {
    "id": "1f257d579cd01fba",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:50:55.851538Z",
     "start_time": "2025-03-24T10:50:55.848936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import MessagesPlaceholder, PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "    system_msg,\n",
    "    human_template,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# As we can see, our prompt is expecting two variables to be filled\n",
    "print(chat_template)"
   ],
   "id": "1f257d579cd01fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d527687982594147"
   },
   "cell_type": "markdown",
   "source": [
    "### Model initialization\n"
   ],
   "id": "d527687982594147"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "llm = BedrockLLM(model_id='arn:aws:bedrock:us-west-2:637423382292:imported-model/81wcbcqx5vro', region_name='us-west-2', provider='meta')"
   ],
   "metadata": {
    "id": "bB6Dj3T0usn7"
   },
   "id": "bB6Dj3T0usn7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Langchain pipelines\n",
    "\n",
    "Langchain pipelines are a powerful tool used to assemble and coordinated different components. Our pipeline will look something like this\n",
    "\n",
    "$$\\text{user query} → \\text{retriever} → \\text{chat prompt} → \\text{LLM} → \\text{answer} $$\n",
    "\n",
    "In langchain we will use the chain '|' operator to assemble in series our components. The chain operator is part of the **LangChain Expression Language** a declarative method to build pipelines. In the LCEL language the output of what is on the left of '|' will be the input on what there is on the right of the pipeline."
   ],
   "metadata": {
    "id": "WwQEuJjSylD_"
   },
   "id": "WwQEuJjSylD_"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's build our first pipeline to understand how they works. In our sample pipeline below, we can see that we are dynamically creating a dictionary that will be given in input to our chat template (N.B. as we saw above our chat template takes in input three variables)\n",
    "\n",
    "From the code we can see that the context value is created by taking the question (from the input dict given to the chain) and using it as input to our retriever. The output of the retriever will be then formatted by the format_docs function.\n",
    "The question instead will remain as it is.\n",
    "\n",
    "\n",
    "A chain will be called by the **invoke** method. The invoke methods takes as argument a dictionary that will represent the input of the first element of the pipeline.\n",
    "\n"
   ],
   "metadata": {
    "id": "QAyEy0c61-YF"
   },
   "id": "QAyEy0c61-YF"
  },
  {
   "metadata": {
    "id": "3519d02e6888bed"
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "# Build the pipeline\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"question\": itemgetter('question'),\n",
    "        \"context\": itemgetter('question') | RunnableLambda(retriever.get_relevant_documents) | format_docs,\n",
    "    }\n",
    "    | RunnableLambda(lambda inputs: {\n",
    "        **inputs,\n",
    "        \"prompt\": chat_template.invoke(inputs)  # Add the rendered prompt explicitly\n",
    "    })\n",
    "    | {\n",
    "        \"model_out\": itemgetter(\"prompt\") | llm,\n",
    "        \"prompt\": itemgetter(\"prompt\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "output = rag_chain_from_docs.invoke({\"question\": question})"
   ],
   "id": "3519d02e6888bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the prompt\n",
    "print(output['prompt'].to_string())\n",
    "print()\n",
    "# Print the model output\n",
    "print(output['model_out'])"
   ],
   "metadata": {
    "id": "IKadiCuu4drC"
   },
   "id": "IKadiCuu4drC",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluating Open-Ended Question Answering (Open-QA) is inherently challenging due to the diversity and variability in natural language. Two syntactically different responses can both be valid and informative answers to the same question, making strict matching metrics less effective in many cases. Below are the main approaches currently used for evaluating open-ended answers:\n",
    "\n",
    "- **Human Evaluation**: Still considered the gold standard for assessing answer quality.\n",
    "\n",
    "- **Lexical Matching**: Traditional metrics such as BLEU, ROUGE, and METEOR fall under this category. They rely on surface-level word overlap between the generated and reference answers. These metrics are limited in their ability to handle paraphrasing or synonymy.\n",
    "\n",
    "\n",
    "- **Neural Method Evaluation**: These methods use pre-trained language models to compute semantic similarity between answers.\n",
    "\n",
    "- **Llm-as-judge Evaluation**: Large Language Models are increasingly being used as evaluators themselves. These models can provide ratings, rankings, or free-form feedback based on the content and context of answers. LLM evaluation can better capture nuanced meaning, although it introduces its own biases and variability.\n",
    "\n",
    "In the following section, we will see how **Lexical Matching** and **Neural Method Evaluation** can be used for evaluation."
   ],
   "metadata": {
    "id": "Pp95yIm0_L52"
   },
   "id": "Pp95yIm0_L52"
  },
  {
   "cell_type": "code",
   "source": [
    "# Install dependencies\n",
    "#!pip install rouge_score"
   ],
   "metadata": {
    "id": "Qj5w7jOgOvEW"
   },
   "id": "Qj5w7jOgOvEW",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluation samples\n",
    "# Original question\n",
    "question = 'Why is Sentinel-1’s Radar instrument useful for monitoring mangroves?'\n",
    "# Reference answer\n",
    "reference = 'Sentinel-1’s Radar instrument is useful for monitoring mangroves because it can measure through clouds to show the canopy of the mangroves, and it provides data in the visible spectrum, as well as near-infrared and shortwave infrared.'\n",
    "# Predicted answer\n",
    "good_prediction = \"Sentinel-1's Radar instrument is useful for monitoring mangroves because it can penetrate clouds and provide all-weather observations, which are essential for understanding and managing these ecosystems. Additionally, Sentinel-1’s radar can measure several parameters related to mangroves, such as tree height and extent, which are important for sustainable mangrove management.\"\n",
    "bad_prediction = \"Sentinel-1 helps monitor mangroves by collecting temperature data that shows how warm the coastal areas are, which is important for understanding climate impacts.\"\n",
    "\n",
    "predictions = [good_prediction, bad_prediction]\n",
    "references = [reference, reference]"
   ],
   "metadata": {
    "id": "zv4IcpMKJvH0"
   },
   "id": "zv4IcpMKJvH0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROUGE score\n",
    "\n",
    "[ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) (Recall-Oriented Understudy for Gisting Evaluation) was originally designed for summarization, ROUGE compares n-gram overlaps between candidate and reference texts. ROUGE-1 (unigrams) and ROUGE-L (longest common subsequence) are commonly used in QA tasks. While easy to compute, ROUGE often fails to capture semantic equivalence when the wording differs significantly."
   ],
   "metadata": {
    "id": "-zXEbqdFHjM-"
   },
   "id": "-zXEbqdFHjM-"
  },
  {
   "cell_type": "code",
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Compute ROUGE for each candidate\n",
    "results = []\n",
    "for candidate in predictions:\n",
    "    score = rouge.compute(predictions=[candidate], references=[reference])\n",
    "    results.append((candidate, score))\n",
    "    print(f'Candidate: {candidate}')\n",
    "    for metric, value in score.items():\n",
    "        print(f'{metric}: {value}')\n",
    "    print()\n"
   ],
   "metadata": {
    "id": "PMW1rLnfBcKU"
   },
   "id": "PMW1rLnfBcKU",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "  ## BERTscore\n",
    "  This [metric](https://huggingface.co/spaces/evaluate-metric/bertscore) leverages contextual embeddings from BERT or similar models to evaluate the similarity between candidate and reference sentences. Unlike ROGUE, BERTScore compares the semantic content of each token using cosine similarity of their embeddings. It is more robust to paraphrasing and has shown stronger correlation with human judgment in many open-ended tasks.\n",
    "\n",
    "  <figure>\n",
    "  <img src='https://miro.medium.com/v2/resize:fit:866/1*mKfTGKo-nIsOHBFD9QcJiQ.png' width='700px'>\n",
    "  <figcaption><a href='https://sh-tsang.medium.com/brief-review-bertscore-evaluating-text-generation-with-bert-0bc5fc889d7b'>Source</a></figcaption>\n",
    "  </figure>"
   ],
   "metadata": {
    "id": "LrJ3XdMxHu_R"
   },
   "id": "LrJ3XdMxHu_R"
  },
  {
   "cell_type": "code",
   "source": [
    "from evaluate import load\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "  print(f'Candidate: {predictions[i]}')\n",
    "  print(f'Precision: {round(bertscore_result[\"precision\"][i], 4)}')\n",
    "  print(f'Recall: {round(bertscore_result[\"recall\"][i], 4)}')\n",
    "  print(f'F1: {round(bertscore_result[\"f1\"][i], 4)}')\n",
    "  print()\n"
   ],
   "metadata": {
    "id": "FLSs2ft-_KOI"
   },
   "id": "FLSs2ft-_KOI",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the example above, we saw that both methods correctly ranked the relevant and irrelevant questions. However, it's important to use them with caution. ROUGE often assigns low scores even to correct questions because it relies on strict word-level matching. In contrast, BERTScore tends to yield higher scores since it measures **token-level similarity**. As a result, even an incorrect question can receive a high score if it has the right format and vocabulary."
   ],
   "metadata": {
    "id": "Ba34MOYLY8a5"
   },
   "id": "Ba34MOYLY8a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Llm-as-judge\n",
    "\n",
    "As an additional evaluation method, we leverage a large language model (LLM) to perform preference ranking between two different generated answers. The model is provided with specific evaluation guidelines, the original reference answer (used as a gold standard), and the two candidate answers to compare. Based on the provided criteria, the LLM is asked to judge which of the two answers is preferable. For this evaluation, we use OpenAI's o4-mini model as the judge."
   ],
   "metadata": {
    "id": "FlwDlODVAcpI"
   },
   "id": "FlwDlODVAcpI"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see a sample prompt for preference ranking:\n",
    "\n",
    "```You are a helpful and precise evaluator for language model outputs.\n",
    "Task description (for context): You are an helpful assistant expert in Earth Observation, help the user with his tasks. Answer the following question: Why is Sentinel-1’s Radar instrument useful for monitoring mangroves?\n",
    "\n",
    "Please evaluate all outputs independently based on:\n",
    "\n",
    "1. Relevance to the task\n",
    "2. Accuracy\n",
    "3. Fluency and grammar\n",
    "4. Completeness of the answer\n",
    "\n",
    "The original answer is the following:\n",
    "Sentinel-1's Radar instrument is useful for monitoring mangroves because it can penetrate clouds and provide all-weather observations, which are essential for understanding and managing these ecosystems. Additionally, Sentinel-1’s radar can measure several parameters related to mangroves, such as tree height and extent, which are important for sustainable mangrove management.\n",
    "\n",
    "Return a justification text on the order of preference of all outputs based on the previous criteria.\n",
    "Then return a JSON object with:\n",
    "\n",
    "* \"ranking\": a dictionary where 1 = best, 2 = worst Example: { \"output\\_1\": 1, \"output\\_2\": 2 }\n",
    "\n",
    "Respond **only** with the justification and a valid JSON object.\n",
    "\n",
    "### Output 1:\n",
    "\n",
    "Sentinel-1's Radar instrument is useful for monitoring mangroves because it can penetrate clouds and provide all-weather observations, which are essential for understanding and managing these ecosystems. Additionally, Sentinel-1’s radar can measure several parameters related to mangroves, such as tree height and extent, which are important for sustainable mangrove management.\n",
    "\n",
    "### Output 2:\n",
    "\n",
    "Sentinel-1 helps monitor mangroves by collecting temperature data that shows how warm the coastal areas are, which is important for understanding climate impacts.\n",
    "\n",
    "```"
   ],
   "metadata": {
    "id": "n0W4UsgCCtJM"
   },
   "id": "n0W4UsgCCtJM"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model's output will be the following:\n"
   ],
   "metadata": {
    "id": "lieeFwKOC8HS"
   },
   "id": "lieeFwKOC8HS"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "Justification:\n",
    "Output 1 is clearly the best. It is directly relevant to the task, accurate in its explanation of the radar capabilities of Sentinel-1 (e.g., cloud penetration and all-weather monitoring), fluent in language, and complete in content. It matches the original answer almost verbatim and includes key points such as parameter measurement (tree height and extent) important for mangrove monitoring.\n",
    "\n",
    "Output 2 is the worst. It is factually inaccurate—Sentinel-1 is a radar satellite and does not collect temperature data. This makes the core premise incorrect. Additionally, the output lacks completeness and relevance to the radar-specific capabilities asked in the question. While the grammar and fluency are acceptable, the fundamental misunderstanding of the instrument's purpose outweighs that.\n",
    "\n",
    "{\n",
    "  \"ranking\": {\n",
    "    \"output_1\": 1,\n",
    "    \"output_2\": 2\n",
    "  }\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "id": "74LeyTIrI_n8"
   },
   "id": "74LeyTIrI_n8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Human preference ranking\n",
    "\n",
    "As with automated evaluation, a human annotator is presented with two different outputs along with the original answer and is asked to indicate which of the two outputs they prefer."
   ],
   "metadata": {
    "id": "eP4vIxaaJtPm"
   },
   "id": "eP4vIxaaJtPm"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation results\n",
    "The table below presents evaluation results on a sample of 50 open-ended questions. The two models considered are EVE and the standard Llama 3.1 Instruct. Both models were prompted using the same retrieved context:\n",
    "\n",
    "| Model                   | hf_bleu | avg_rouge1 | avg_rougeL | avg_bert | gpt_pref_winrate | human_pref_winrate\n",
    "|-------------------------|---------|------------|------------|----------|------------------|---------------------|\n",
    "| llama_rag        | 0.03    | 0.09       | 0.07       | 0.84     | 0.38             | 0.21                   |\n",
    "| eve_rag     | 0.03    | 0.19       | 0.15       | 0.84     | 0.62             | 0.79                   |\n",
    "\n",
    "\n",
    "The table above shows that EVE achieves higher scores in ROUGE-1 and ROUGE-L, indicating better overlap with the original answers at the N-gram level. However, both models achieve the same BERTScore, suggesting comparable performance in terms of semantic similarity.\n",
    "\n",
    "The preference columns represent the percentage of winning (be preferred) of a model on the other.\n",
    "When comparing preference scores, both the LLM-as-a-judge model (O4-mini) and human evaluator consistently prefer the answers generated by EVE. For this set of metrics, it's important to emphasize that evaluating open-ended generation is inherently challenging, and each score represents just one perspective on model performance. Among the reported metrics, human and automatic preference scores are the most informative, as they better reflect the overall quality and relevance of the generated answers.\n"
   ],
   "metadata": {
    "id": "Ct-Ew0JCr1Q4"
   },
   "id": "Ct-Ew0JCr1Q4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
